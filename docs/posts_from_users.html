<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>src.posts_from_users API documentation</title>
<meta name="description" content="Script para la extracción e indexado de posts de Reddit a partir de una lista de usuarios
…" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.posts_from_users</code></h1>
</header>
<section id="section-intro">
<h2 id="script-para-la-extraccion-e-indexado-de-posts-de-reddit-a-partir-de-una-lista-de-usuarios">Script Para La Extracción E Indexado De Posts De Reddit A Partir De Una Lista De Usuarios</h2>
<p>Recupera el histórico de posts de una lista de usuarios y "gemelos" previos a la fecha especificada
a través de la API de Pushshift.
Los documentos se indexan en un servidor Elasticsearch. Los posts se marcan con un campo booleano
en función de si pertenecen a usuarios o a sus "gemelos".
Los posts se vuelcan en un fichero .ndjson además de ser indexados, para poseer un backup.</p>
<p>El fichero de entrada es un .csv donde cada fila es un usuario. Este csv debe tener al menos los campos
"Usuario" y "Muestra", para poder extraer el nombre de los usuarios y etiquetar sus posts de forma adecuada.</p>
<h2 id="parametros">Parámetros</h2>
<ul>
<li>-u, &ndash;users: fichero .csv con la lista de usuarios</li>
<li>-d, &ndash;dump-dir: directorio donde se volcará el fichero .ndjson. Por defecto /dumps</li>
<li>-s, &ndash;subreddits: Fichero con los subreddit a excluir</li>
<li>-e, &ndash;elasticsearch: dirección del servidor Elasticsearch contra el que se indexará. Por defecto <a href="http://localhost:9200">http://localhost:9200</a></li>
<li>-b, &ndash;before: fecha donde se comenzará a extraer posts hacia atrás en el tiempo. Por defecto, la fecha actual.</li>
</ul>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
    Script para la extracción e indexado de posts de Reddit a partir de una lista de usuarios
    -----------------------------------------------------------------------------------------

    Recupera el histórico de posts de una lista de usuarios y &#34;gemelos&#34; previos a la fecha especificada
    a través de la API de Pushshift.
    Los documentos se indexan en un servidor Elasticsearch. Los posts se marcan con un campo booleano
    en función de si pertenecen a usuarios o a sus &#34;gemelos&#34;.
    Los posts se vuelcan en un fichero .ndjson además de ser indexados, para poseer un backup.

    El fichero de entrada es un .csv donde cada fila es un usuario. Este csv debe tener al menos los campos
    &#34;Usuario&#34; y &#34;Muestra&#34;, para poder extraer el nombre de los usuarios y etiquetar sus posts de forma adecuada.

    Parámetros
    ----------
    * -u, --users: fichero .csv con la lista de usuarios
    * -d, --dump-dir: directorio donde se volcará el fichero .ndjson. Por defecto /dumps
    * -s, --subreddits: Fichero con los subreddit a excluir
    * -e, --elasticsearch: dirección del servidor Elasticsearch contra el que se indexará. Por defecto http://localhost:9200
    * -b, --before: fecha donde se comenzará a extraer posts hacia atrás en el tiempo. Por defecto, la fecha actual.

&#34;&#34;&#34;
from psaw import PushshiftAPI
import argparse
from datetime import datetime as dt
from datetime import date
import json
import os
from elasticsearch import Elasticsearch
from src.elastic_utils.elastic_indexers import Indexer, NgramIndexer
import progressbar as pb

__author__ = &#34;Samuel Cifuentes García&#34;


def main(args):
    # Establecemos conexión a Elastic
    global es
    es = Elasticsearch(args.elasticsearch)

    # Inicializamos el cliente de la API
    global api
    api = PushshiftAPI()

    print(&#34;Cargando usuarios...&#34;)
    users = load_users(args.users)

    if args.subreddits:
        print(&#34;Cargando subreddits a excluir...&#34;)
        subreddits = load_subreddits(args.subreddits)
    else:
        subreddits = [&#34;lonely&#34;]
    
    if not os.path.exists(args.dump_dir):
        os.makedirs(args.dump_dir)
    dump_filename = args.dump_dir + &#34;/user_posts-Dump.ndjson&#34;

    print(&#34;Obteniendo e indexando posts...&#34;)
    query_api(users, args.before, subreddits, filename = dump_filename)


def load_users(path):
    &#34;&#34;&#34;
        Carga los usuarios desde un .csv y crea un diccionario con sus nombres de usuario
        y la muestra a la que pertenecen

        Parámetros
        ----------
        path: str  
            \tRuta del archivo .csv con los usuarios

        Salida
        ------
        dict  
            \tDiccionario con los nombres de usuario y sus respectivas muestras
    &#34;&#34;&#34;
    users = {}
    with open(path) as f:
        headers = next(f).split(&#34;;&#34;)
        index_name = headers.index(&#34;Usuario&#34;)
        index_group = headers.index(&#34;Muestra&#34;) # Necesitamos este campo para marcar el usuario como control o lonely

        for line in f:
          data = line.split(&#34;;&#34;)
          users[data[index_name]] = data[index_group]
    return users  

def query_api(users, before_date, subreddits, filename=&#34;dumps/user_posts-Dump.ndjson&#34;, cache_size=10000):
    &#34;&#34;&#34;
        Recupera los post de una lista de usuarios, los indexa en Elastic y vuelca a un fichero a modo
        de backup.  
        Se excluyen los posts que pertenezcan añ subreddit pasados por parámetro.

        Parámetros
        ----------
        users: dict  
            \tDiccionario con nombres de usuario y muestras a las que pertenecen  
        before_date: date  
            \tFecha límite tras la cuál no se extraerán más documentos  
        subreddit: str
            \tNombre del subreddit a excluir  
        filename: str  
            \tNombre del fichero donde se volcarán los documentos  
        cache_size: int  
            \tTamaño de los bloques de documentos que se indexarán de cada vez. Regular en función
            de la memoria disponible
    &#34;&#34;&#34;
    # Barra de progreso
    bar = pb.ProgressBar(max_value=pb.UnknownLength, widgets=[
        &#34;- &#34;, pb.AnimatedMarker(), &#34; Docs processed: &#34;, pb.Counter(), &#34; &#34;, pb.Timer()
    ])
    num_iter = 0
    # Ahorramos tiempo de máquina consultando a la API de 100 en 100 usuarios
    # Más no es viable porque superamos los límites de caracteres de la petición GET
    user_block = []
    for user in users:
        user_block.append(user)

        if len(user_block) == 100:
            # Se extraen todos los posts del usuario
            gen = api.search_submissions(author=user_block, before=int(before_date.timestamp()))

            # El tamaño de la caché dependerá de la memoria que tengamos
            cache = []
            for c in gen:
                # Me tiene pasado que me lleguen posts sin subreddit, de ahí la primera condición
                # Excluimos posts en el subreddit pasado por parámetro
                if &#34;subreddit&#34; in c.d_ and c.d_[&#34;subreddit&#34;] in subreddits:
                    # Usamos el campo muestra del .csv para marcar los posts como lonely o no
                    c.d_[&#34;lonely&#34;] = users[c.d_[&#34;author&#34;]] == &#34;lonely&#34;        
                    cache.append(c.d_)

                if len(cache) == cache_size:      
                    dump_to_file(cache, filename)
                    elastic_index(cache)
                    
                    cache = []
                
                # Actualizar barra
                num_iter += 1
                bar.update(num_iter)                

            # Los restantes al salir del bucle
            dump_to_file(cache, filename)
            elastic_index(cache)

            user_block = []

def load_subreddits(path):
    &#34;&#34;&#34;
        Carga una lista de subreddits de un archivo en disco.
        El archivo debe ser un fichero de texto con un subreddit en cada línea.

        Parámetros
        ----------
        path: str  
            \tRuta del fichero con los subreddits

        Salida
        ------
        list  
            \tLista de subreddits
    &#34;&#34;&#34;
    subreddits = []
    with open(path) as f:
        for sub in f:
            subreddits.append(sub)
    return subreddits


def dump_to_file(results, path):
    &#34;&#34;&#34;
        Vuelca una lista de submissions a un fichero .ndjson. Los volcados se deben realizar de forma parcial 
        debido a limitaciones de memoria.
        Las escrituras son mucho más rápidas si se tratan como strings en vez de objetos JSON.

        Parámetros
        ----------
        results: list
            lista de documentos a volcar   
    &#34;&#34;&#34;
    with open(path, &#34;a&#34;) as f:
        for result in results:
            f.write(json.dumps(result) + &#34;\n&#34;)

def elastic_index(results):
    &#34;&#34;&#34;
        Indexa la lista de documentos pasado por parámetro.  
        A cada documento se le añadió un campo booleano para identificar si pertenece a un usuario que haya
        participado en el subreddit objetivo o uno de sus gemelos.

        Parámetros
        ----------
        results: list
            lista de documentos a indexar
    &#34;&#34;&#34;
    indexer = Indexer(es, &#34;phase-b&#34;)
    if not indexer.index_exists():
        print(&#34;Creado índice: &#34; + indexer.index_name)
        indexer.create_index()
        
    indexer.index_documents(results)

def parse_args():
    &#34;&#34;&#34;
        Procesamiento de los argumentos con los que se ejecutó el script
    &#34;&#34;&#34;
    parser = argparse.ArgumentParser(description=&#34;Script para la extracción de submissions de Reddit a partir de una lista de usuarios&#34;)
    parser.add_argument(&#34;-u&#34;, &#34;--users&#34;, default=&#34;csv/users_data.csv&#34;, help=&#34;Fichero con los usuarios&#34;)
    parser.add_argument(&#34;-d&#34;, &#34;--dump-dir&#34;, default=&#34;dumps&#34;, help=&#34;Directorio donde se volcará el archivo .ndjson de backup&#34;)
    parser.add_argument(&#34;-s&#34;, &#34;--subreddits&#34;, help=&#34;Fichero con los subreddits a excluir&#34;)
    parser.add_argument(&#34;-e&#34;, &#34;--elasticsearch&#34;, default=&#34;http://localhost:9200&#34;, help=&#34;Dirección del servidor Elasticsearch contra el que se indexará&#34;)
    parser.add_argument(&#34;-b&#34;, &#34;--before&#34;, default=dt.now(), type= lambda d: dt.strptime(d + &#34; 23:59:59&#34;, &#39;%Y-%m-%d %H:%M:%S&#39;),
        help=&#34;Fecha desde la que se empezará a recuperar documentos hacia atrás en formato YYYY-mm-dd&#34;)
    return parser.parse_args()

if __name__ == &#34;__main__&#34;:
    main(parse_args())</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="src.posts_from_users.dump_to_file"><code class="name flex">
<span>def <span class="ident">dump_to_file</span></span>(<span>results, path)</span>
</code></dt>
<dd>
<div class="desc"><p>Vuelca una lista de submissions a un fichero .ndjson. Los volcados se deben realizar de forma parcial
debido a limitaciones de memoria.
Las escrituras son mucho más rápidas si se tratan como strings en vez de objetos JSON.</p>
<h2 id="parametros">Parámetros</h2>
<p>results: list
lista de documentos a volcar</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dump_to_file(results, path):
    &#34;&#34;&#34;
        Vuelca una lista de submissions a un fichero .ndjson. Los volcados se deben realizar de forma parcial 
        debido a limitaciones de memoria.
        Las escrituras son mucho más rápidas si se tratan como strings en vez de objetos JSON.

        Parámetros
        ----------
        results: list
            lista de documentos a volcar   
    &#34;&#34;&#34;
    with open(path, &#34;a&#34;) as f:
        for result in results:
            f.write(json.dumps(result) + &#34;\n&#34;)</code></pre>
</details>
</dd>
<dt id="src.posts_from_users.elastic_index"><code class="name flex">
<span>def <span class="ident">elastic_index</span></span>(<span>results)</span>
</code></dt>
<dd>
<div class="desc"><p>Indexa la lista de documentos pasado por parámetro.<br>
A cada documento se le añadió un campo booleano para identificar si pertenece a un usuario que haya
participado en el subreddit objetivo o uno de sus gemelos.</p>
<h2 id="parametros">Parámetros</h2>
<p>results: list
lista de documentos a indexar</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def elastic_index(results):
    &#34;&#34;&#34;
        Indexa la lista de documentos pasado por parámetro.  
        A cada documento se le añadió un campo booleano para identificar si pertenece a un usuario que haya
        participado en el subreddit objetivo o uno de sus gemelos.

        Parámetros
        ----------
        results: list
            lista de documentos a indexar
    &#34;&#34;&#34;
    indexer = Indexer(es, &#34;phase-b&#34;)
    if not indexer.index_exists():
        print(&#34;Creado índice: &#34; + indexer.index_name)
        indexer.create_index()
        
    indexer.index_documents(results)</code></pre>
</details>
</dd>
<dt id="src.posts_from_users.load_subreddits"><code class="name flex">
<span>def <span class="ident">load_subreddits</span></span>(<span>path)</span>
</code></dt>
<dd>
<div class="desc"><p>Carga una lista de subreddits de un archivo en disco.
El archivo debe ser un fichero de texto con un subreddit en cada línea.</p>
<h2 id="parametros">Parámetros</h2>
<p>path: str<br>
Ruta del fichero con los subreddits</p>
<h2 id="salida">Salida</h2>
<p>list<br>
Lista de subreddits</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_subreddits(path):
    &#34;&#34;&#34;
        Carga una lista de subreddits de un archivo en disco.
        El archivo debe ser un fichero de texto con un subreddit en cada línea.

        Parámetros
        ----------
        path: str  
            \tRuta del fichero con los subreddits

        Salida
        ------
        list  
            \tLista de subreddits
    &#34;&#34;&#34;
    subreddits = []
    with open(path) as f:
        for sub in f:
            subreddits.append(sub)
    return subreddits</code></pre>
</details>
</dd>
<dt id="src.posts_from_users.load_users"><code class="name flex">
<span>def <span class="ident">load_users</span></span>(<span>path)</span>
</code></dt>
<dd>
<div class="desc"><p>Carga los usuarios desde un .csv y crea un diccionario con sus nombres de usuario
y la muestra a la que pertenecen</p>
<h2 id="parametros">Parámetros</h2>
<p>path: str<br>
Ruta del archivo .csv con los usuarios</p>
<h2 id="salida">Salida</h2>
<p>dict<br>
Diccionario con los nombres de usuario y sus respectivas muestras</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_users(path):
    &#34;&#34;&#34;
        Carga los usuarios desde un .csv y crea un diccionario con sus nombres de usuario
        y la muestra a la que pertenecen

        Parámetros
        ----------
        path: str  
            \tRuta del archivo .csv con los usuarios

        Salida
        ------
        dict  
            \tDiccionario con los nombres de usuario y sus respectivas muestras
    &#34;&#34;&#34;
    users = {}
    with open(path) as f:
        headers = next(f).split(&#34;;&#34;)
        index_name = headers.index(&#34;Usuario&#34;)
        index_group = headers.index(&#34;Muestra&#34;) # Necesitamos este campo para marcar el usuario como control o lonely

        for line in f:
          data = line.split(&#34;;&#34;)
          users[data[index_name]] = data[index_group]
    return users  </code></pre>
</details>
</dd>
<dt id="src.posts_from_users.main"><code class="name flex">
<span>def <span class="ident">main</span></span>(<span>args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def main(args):
    # Establecemos conexión a Elastic
    global es
    es = Elasticsearch(args.elasticsearch)

    # Inicializamos el cliente de la API
    global api
    api = PushshiftAPI()

    print(&#34;Cargando usuarios...&#34;)
    users = load_users(args.users)

    if args.subreddits:
        print(&#34;Cargando subreddits a excluir...&#34;)
        subreddits = load_subreddits(args.subreddits)
    else:
        subreddits = [&#34;lonely&#34;]
    
    if not os.path.exists(args.dump_dir):
        os.makedirs(args.dump_dir)
    dump_filename = args.dump_dir + &#34;/user_posts-Dump.ndjson&#34;

    print(&#34;Obteniendo e indexando posts...&#34;)
    query_api(users, args.before, subreddits, filename = dump_filename)</code></pre>
</details>
</dd>
<dt id="src.posts_from_users.parse_args"><code class="name flex">
<span>def <span class="ident">parse_args</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Procesamiento de los argumentos con los que se ejecutó el script</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse_args():
    &#34;&#34;&#34;
        Procesamiento de los argumentos con los que se ejecutó el script
    &#34;&#34;&#34;
    parser = argparse.ArgumentParser(description=&#34;Script para la extracción de submissions de Reddit a partir de una lista de usuarios&#34;)
    parser.add_argument(&#34;-u&#34;, &#34;--users&#34;, default=&#34;csv/users_data.csv&#34;, help=&#34;Fichero con los usuarios&#34;)
    parser.add_argument(&#34;-d&#34;, &#34;--dump-dir&#34;, default=&#34;dumps&#34;, help=&#34;Directorio donde se volcará el archivo .ndjson de backup&#34;)
    parser.add_argument(&#34;-s&#34;, &#34;--subreddits&#34;, help=&#34;Fichero con los subreddits a excluir&#34;)
    parser.add_argument(&#34;-e&#34;, &#34;--elasticsearch&#34;, default=&#34;http://localhost:9200&#34;, help=&#34;Dirección del servidor Elasticsearch contra el que se indexará&#34;)
    parser.add_argument(&#34;-b&#34;, &#34;--before&#34;, default=dt.now(), type= lambda d: dt.strptime(d + &#34; 23:59:59&#34;, &#39;%Y-%m-%d %H:%M:%S&#39;),
        help=&#34;Fecha desde la que se empezará a recuperar documentos hacia atrás en formato YYYY-mm-dd&#34;)
    return parser.parse_args()</code></pre>
</details>
</dd>
<dt id="src.posts_from_users.query_api"><code class="name flex">
<span>def <span class="ident">query_api</span></span>(<span>users, before_date, subreddits, filename='dumps/user_posts-Dump.ndjson', cache_size=10000)</span>
</code></dt>
<dd>
<div class="desc"><p>Recupera los post de una lista de usuarios, los indexa en Elastic y vuelca a un fichero a modo
de backup.<br>
Se excluyen los posts que pertenezcan añ subreddit pasados por parámetro.</p>
<h2 id="parametros">Parámetros</h2>
<p>users: dict<br>
Diccionario con nombres de usuario y muestras a las que pertenecen<br>
before_date: date<br>
Fecha límite tras la cuál no se extraerán más documentos<br>
subreddit: str
Nombre del subreddit a excluir<br>
filename: str<br>
Nombre del fichero donde se volcarán los documentos<br>
cache_size: int<br>
Tamaño de los bloques de documentos que se indexarán de cada vez. Regular en función
de la memoria disponible</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def query_api(users, before_date, subreddits, filename=&#34;dumps/user_posts-Dump.ndjson&#34;, cache_size=10000):
    &#34;&#34;&#34;
        Recupera los post de una lista de usuarios, los indexa en Elastic y vuelca a un fichero a modo
        de backup.  
        Se excluyen los posts que pertenezcan añ subreddit pasados por parámetro.

        Parámetros
        ----------
        users: dict  
            \tDiccionario con nombres de usuario y muestras a las que pertenecen  
        before_date: date  
            \tFecha límite tras la cuál no se extraerán más documentos  
        subreddit: str
            \tNombre del subreddit a excluir  
        filename: str  
            \tNombre del fichero donde se volcarán los documentos  
        cache_size: int  
            \tTamaño de los bloques de documentos que se indexarán de cada vez. Regular en función
            de la memoria disponible
    &#34;&#34;&#34;
    # Barra de progreso
    bar = pb.ProgressBar(max_value=pb.UnknownLength, widgets=[
        &#34;- &#34;, pb.AnimatedMarker(), &#34; Docs processed: &#34;, pb.Counter(), &#34; &#34;, pb.Timer()
    ])
    num_iter = 0
    # Ahorramos tiempo de máquina consultando a la API de 100 en 100 usuarios
    # Más no es viable porque superamos los límites de caracteres de la petición GET
    user_block = []
    for user in users:
        user_block.append(user)

        if len(user_block) == 100:
            # Se extraen todos los posts del usuario
            gen = api.search_submissions(author=user_block, before=int(before_date.timestamp()))

            # El tamaño de la caché dependerá de la memoria que tengamos
            cache = []
            for c in gen:
                # Me tiene pasado que me lleguen posts sin subreddit, de ahí la primera condición
                # Excluimos posts en el subreddit pasado por parámetro
                if &#34;subreddit&#34; in c.d_ and c.d_[&#34;subreddit&#34;] in subreddits:
                    # Usamos el campo muestra del .csv para marcar los posts como lonely o no
                    c.d_[&#34;lonely&#34;] = users[c.d_[&#34;author&#34;]] == &#34;lonely&#34;        
                    cache.append(c.d_)

                if len(cache) == cache_size:      
                    dump_to_file(cache, filename)
                    elastic_index(cache)
                    
                    cache = []
                
                # Actualizar barra
                num_iter += 1
                bar.update(num_iter)                

            # Los restantes al salir del bucle
            dump_to_file(cache, filename)
            elastic_index(cache)

            user_block = []</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#script-para-la-extraccion-e-indexado-de-posts-de-reddit-a-partir-de-una-lista-de-usuarios">Script para la extracción e indexado de posts de Reddit a partir de una lista de usuarios</a></li>
<li><a href="#parametros">Parámetros</a></li>
</ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src" href="index.html">src</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="src.posts_from_users.dump_to_file" href="#src.posts_from_users.dump_to_file">dump_to_file</a></code></li>
<li><code><a title="src.posts_from_users.elastic_index" href="#src.posts_from_users.elastic_index">elastic_index</a></code></li>
<li><code><a title="src.posts_from_users.load_subreddits" href="#src.posts_from_users.load_subreddits">load_subreddits</a></code></li>
<li><code><a title="src.posts_from_users.load_users" href="#src.posts_from_users.load_users">load_users</a></code></li>
<li><code><a title="src.posts_from_users.main" href="#src.posts_from_users.main">main</a></code></li>
<li><code><a title="src.posts_from_users.parse_args" href="#src.posts_from_users.parse_args">parse_args</a></code></li>
<li><code><a title="src.posts_from_users.query_api" href="#src.posts_from_users.query_api">query_api</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>