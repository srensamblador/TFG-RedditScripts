<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.1" />
<title>src.classify.train API documentation</title>
<meta name="description" content="Script para entrenar un clasificador de post de Reddit
…" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.classify.train</code></h1>
</header>
<section id="section-intro">
<h2 id="script-para-entrenar-un-clasificador-de-post-de-reddit">Script Para Entrenar Un Clasificador De Post De Reddit</h2>
<h2 id="parametros">Parámetros</h2>
<ul>
<li>training: fichero con el dataset de entrenamiento</li>
<li>testing: fichero con el dataset de test</li>
<li>-v, &ndash;vocab-size: Número de términos más frecuentes a incluir en el vocabulario. </li>
<li>-s, &ndash;seed: semilla a utilizar para la generación de números pseudoaleatorios. Opcional.</li>
<li>&ndash;stem: aplica estemetización en el preprocesado de texto</li>
<li>&ndash;no-stem: no aplica estemetización en el preprocesado de texto</li>
</ul>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;
    Script para entrenar un clasificador de post de Reddit
    ------------------------------------------------------

    Parámetros
    ----------
    * training: fichero con el dataset de entrenamiento
    * testing: fichero con el dataset de test
    * -v, --vocab-size: Número de términos más frecuentes a incluir en el vocabulario. 
    * -s, --seed: semilla a utilizar para la generación de números pseudoaleatorios. Opcional.
    * --stem: aplica estemetización en el preprocesado de texto
    * --no-stem: no aplica estemetización en el preprocesado de texto
&#34;&#34;&#34;

import os
import json
import string
import numpy as np
import progressbar as pb
import re
import argparse
import pickle
import joblib
from pprint import pprint
from datetime import datetime as dt

from nltk.stem import PorterStemmer
from collections import Counter
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn import svm
from scipy import sparse as sp

from src.classify.model_stats import get_stats

__author__=&#34;Samuel Cifuentes García&#34;

ps = PorterStemmer()
VOCAB_SIZE = 5000

def main(args):
    global VOCAB_SIZE
    VOCAB_SIZE = args.vocab_size

    if args.seed:
        np.random.seed(args.seed)
        
    num_training_docs = file_length(args.training)
    
    # Comentando o descomentando los bloques indicados podemos serializar los resultados parciales
    # del preprocesamiento previo al entrenamiento en disco, para ahorrarnos estos pasos en ejecuciones posteriores

    print(&#34;Generando el vocabulario...&#34;)
    vocabulary = create_vocabulary(args.training, num_training_docs, args.stem)    
    # De esta forma, podemos serializar el vocabulario para no tener que recalcularlo cuando entrenemos otros modelos
    joblib.dump(vocabulary, &#34;pickles/vocabulary.pickle&#34;)
    &#34;&#34;&#34;
    # Si tenemos el vocabulario ya en disco, se cargaría así    
    vocabulary = joblib.load(&#34;pickles/vocabulary.pickle&#34;)    
    &#34;&#34;&#34;
    print(&#34;Obteniendo etiquetas...&#34;)
    tags_training = get_tags(args.training, num_training_docs)
    # Serializar etiquetas
    joblib.dump(tags_training, &#34;pickles/tags.pickle&#34;)    
    &#34;&#34;&#34;    
    # Cargarlas desde disco
    tags_training = joblib.load(&#34;pickles/tags.pickle&#34;)
    &#34;&#34;&#34;
    print(&#34;Extrayendo características...&#34;)
    matrix_training = extract_features(args.training, vocabulary, num_training_docs, args.stem)
    # Serializar
    sp.save_npz(&#34;pickles/features.npz&#34;, matrix_training)
    &#34;&#34;&#34;
    # Cargar desde disco
    matrix_training = sp.load_npz(&#34;pickles/features.npz&#34;)
    &#34;&#34;&#34;

    print(&#34;Entrenando modelos...&#34;)
    # Metemos aquí los modelos que queramos entrenar en esta ejecución del script
    models = {
        &#34;LogisticRegression&#34;: LogisticRegression(random_state=args.seed),
        &#34;GiniTree&#34;: DecisionTreeClassifier(random_state=args.seed),
        &#34;IDFTree&#34;: DecisionTreeClassifier(criterion=&#34;entropy&#34;, random_state=args.seed),
        &#34;Bayes&#34;: MultinomialNB(),
        &#34;NN_1L_150&#34;: MLPClassifier(hidden_layer_sizes=(150), random_state=args.seed),
        &#34;NN_2L_150&#34;: MLPClassifier(hidden_layer_sizes=(150,150), random_state=args.seed),
        &#34;NN_1L_300&#34;: MLPClassifier(hidden_layer_sizes=(300), random_state=args.seed),
        &#34;SVM_Linear&#34;: svm.LinearSVC()
    }

    ## Ejemplos
    # model = LogisticRegression(verbose=True)
    # model = MultinomialNB() # Red Bayesiana
    # model = DecisionTreeClassifier()
    # model = DecisionTreeClassifier(criterion=&#34;entropy&#34;)
    # model = MLPClassifier(hidden_layer_sizes=(150), random_state=args.seed, verbose=True)
    # model = svm.LinearSVC(verbose=True)
    # model = svm.SVC(cache_size=10000, verbose=True) # TODO Probar parámetro cache_size
    
    # Para entrenar los modelos secuencialmente
    train_models(models, matrix_training, tags_training, n_jobs=8)

    print(&#34;Evaluando modelos...&#34;)
    num_test_docs = file_length(args.test)
    print(&#34;Obteniendo etiquetas del conjunto de test...&#34;)    
    tags_test = get_tags(args.test, num_test_docs)   
    # Serializar
    joblib.dump(tags_test, &#34;pickles/tags_test.pickle&#34;)
    &#34;&#34;&#34;
    # Alternativamente, cargar de disco
    tags_test = joblib.load(&#34;pickles/tags_test.pickle&#34;)
    &#34;&#34;&#34;
    print(&#34;Extrayendo características del conjunto de test...&#34;)   
    matrix_test = extract_features(args.test, vocabulary, num_test_docs, args.stem)
    # Serializar
    sp.save_npz(&#34;pickles/features_test.npz&#34;, matrix_test)    
    &#34;&#34;&#34;
    # Cargar de disco
    matrix_test = sp.load_npz(&#34;pickles/features_test.npz&#34;)    
    &#34;&#34;&#34;

    print(&#34;Generando predicciones para el conjunto de test...&#34;)
    for model in models:
        print(&#34;Modelo &#34; + model + &#34;...&#34;)
        results = models[model].predict(matrix_test)
        pprint(get_stats(results, tags_test))

def train_models(models, features, tags, n_jobs = 1):
    &#34;&#34;&#34;
        Entrena un conjunto de modelos.
        
        Por lo general, los modelos de scikit-learn se entrenan en un único hilo. En su lugar, lo que podemos hacer 
        es entrenar varios modelos, cada uno de ellos en un hilo diferente. Para ello se utiliza el parámetro n_jobs
        de esta función. Si éste es 1, los modelos se entrenarán secuencialmente. Si es mayor de 1, se entrenarán en paralelo
        utilizando tantos hilos como se hayan especificado.

        Parámetros
        ----------
        models: dict  
            \tDiccionario con los modelos a entrenar  
        features: csr_matrix  
            \tMatriz dispersa con las características del conjunto de entrenamiento  
        tags: np.array  
            \tMatriz con las etiquetas del conjunto de entrenamiento  
        n_jobs: int  
            \tNúmero de hilos a utilizar. 
        
        Salida
        ------
        models: dict
            \tDiccionario con los modelos ya entrenados
    &#34;&#34;&#34;
    if n_jobs &gt; 1:
        # [joblib.Parallel](https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html) nos permite entrenar varios modelos concurrentemente
        results = joblib.Parallel(n_jobs=n_jobs)(joblib.delayed(__train_model)(model, models[model], features, tags) for model in models)
        for name, trained_model in results:
            models[name] = trained_model
    else:
        for model in models:
            models[model] = __train_model(model, models[model], features, tags)[1]
    return models

def __train_model(name, model, features, tags):
    &#34;&#34;&#34;
        Entrena un clasificador

        Parámetros
        ----------
        name: str  
            \tNombre con el que identificar el modelo  
        model: obj  
            \tModelo de scikit-learn a entrenar  
        features: csr_matrix  
            \tMatriz dispersa con las características del conjunto de entrenamiento  
        tags: np.array  
            \tMatriz con las etiquetas del conjunto de entrenamiento
        
        Salida
        ------
        name: str  
            \tNombre con el que identificar el modelo
        model: obj  
            \tModelo ya entrenado  
    &#34;&#34;&#34;
    model = model.fit(features, tags)
    joblib.dump(model, &#34;modelos/&#34; + name.lower() + &#34;.pickle&#34;) # Serializamos
    return name, model


def create_vocabulary(path, num_docs, stem):
    &#34;&#34;&#34;
        Genera una lista de n términos más frecuentes en el dataset proporcionado. 
        Los términos de este vocabulario formarán las características de cada individuo del dataset.

        Parámetros
        ----------
        path: str  
            \tRuta del dataset a utilizar
        num_docs: int  
            \tNúmero de documentos del dataset
        stem: bool  
            \tIndica si se aplica o no estemetización en el preprocesado del texto

        Salida
        ------
        list  
            \tLista de n términos más frecuentes del dataset
    &#34;&#34;&#34;
    word_list = []
    with open(path, encoding=&#34;UTF-8&#34;) as f:
        # Barra de progreso
        bar = pb.ProgressBar(max_value=num_docs)
        for line in bar(f):
            data = json.loads(line)[&#34;_source&#34;]
            # Preprocesamos el texto hasta obtener una lista de tokens
            word_list += preprocess_text(data, stem)

    # Nos quedamos con los VOCAB_SIZE términos más frecuentes
    vocabulary = Counter(word_list)
    vocabulary = vocabulary.most_common(VOCAB_SIZE)

    vocabulary_list = []
    for item, _ in vocabulary:
        vocabulary_list.append(item)

    return vocabulary_list

def get_tags(path, num_docs):
    &#34;&#34;&#34;
        Extrae los valores de la etiqueta a clasificar del dataset especificado

        Parámetros
        ----------
        path: str  
            \tRuta del dataset
        num_docs: int  
            \tNúmero de documentos en el dataset

        Salida
        ------
        ndarray  
            \tMatriz con las etiquetas del dataset especificado
    &#34;&#34;&#34;

    # Tipo int16 por limitaciones de memoria
    matrix = np.zeros((num_docs), dtype=np.int16)
    doc_id = 0

    bar = pb.ProgressBar(max_value=num_docs)
    with open(path, encoding=&#34;UTF-8&#34;) as f:
        for line in bar(f):
            tag = json.loads(line)[&#34;_source&#34;][&#34;lonely&#34;]
            matrix[doc_id] = int(tag)
            doc_id +=1
    return matrix

def extract_features(path, vocabulary, num_docs, stem):
    &#34;&#34;&#34;
        Genera la matriz de características del dataset proporcionado.

        Se itera a través de los documentos del dataset, de los que se extraen todos los términos del título y el selftext, 
        tras previo preprocesamiento. Para cada término que esté a su vez en el vocabulario generado anteriormente, se calcula 
        su frecuencia (tf) en el documento.

        Por lo tanto, al final obtenemos una matriz donde cada fila es un documento y cada columna es el tf de uno de los términos del vocabulario.


        Parámetros
        ----------
        path: str  
            \tRuta del dataset
        vocabulary: list  
            \tLista de n términos más frecuentes en el dataset obtenida en create_vocabulary
        num_docs: int  
            \tNúmero de documentos en el dataset
        stem: bool  
            \tIndica si se aplica o no estemetización en el preprocesamiento del texto

        Salida
        ------
        ndarray  
            \tMatriz con el tf de cada palabra en cada documento       
        
    &#34;&#34;&#34;

    &#34;&#34;&#34;
        La matriz de características puede llegar a tener tamaños problemáticos debido a las limitaciones de hardware que pudiéramos tener.
        Por otra parte, la naturaleza de la matriz de características en nuestro problema implica que estamos trabajando con una matriz donde
        la mayor parte de los valores son 0s (cada término del vocabulario que no aparezca en un documento va tener una frecuencia 0)
        Una matriz donde la mayor parte de los valores son 0 es lo que se conoce como matriz dispersa, la cuál podemos almacenar en una estructura 
        de datos especializada, como lo es [csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix) dentro del paquete `scipy`
        Utilizando esta estructura en vez de un array de numpy podemos llegar a ahorrar mucha memoria.
    &#34;&#34;&#34;
    # Necesitamos llevar estos tres arrays para crear la matriz dispersa al acabar el procesado
    indices = [] # Índices de los términos
    values = [] # Las frecuencias
    indptr = [] # Punteros que indican donde empieza cada documento

    bar = pb.ProgressBar(max_value=num_docs)
    indptr.append(0)
    with open(path, encoding=&#34;UTF-8&#34;) as f:
        for line in bar(f):
            data = json.loads(line)[&#34;_source&#34;]
            word_list = preprocess_text(data, stem)

            features = {}
            for word in word_list:
                if word in vocabulary:
                    vocab_index = vocabulary.index(word)
                    if word in features:
                        features[vocab_index] += 1
                    else:
                        features[vocab_index] = 1 
            
            indices.extend(features.keys())
            values.extend(features.values())
            indptr.append(len(indices))

    # Interesante jugar con el tipo numérico para ahorrar memoria. 
    indices = np.asarray(indices, dtype=np.int32)
    indptr = np.asarray(indptr, dtype=np.int32)
    values = np.asarray(values, dtype=np.int32)

    matrix = sp.csr_matrix((values, indices, indptr),
        shape=(len(indptr) -1, len(vocabulary)),
        dtype= np.int32)
    matrix.sort_indices()

    return matrix

def preprocess_text(data, stem):
    &#34;&#34;&#34;
        Se extrae el título y el contenido de un post y se realiza un preprocesado del texto, eliminando
        puntuación, espacios en blanco y si se indica, aplicando un stemmer

        Parámetros
        ----------
        data: json  
            \tDocumento con el contenido de un post de Reddit
        stem: bool  
            \tIndica si se aplica o no estemetización al texto

        Salida
        ------
        list  
            \tLista de palabras del documento preprocesadas
    &#34;&#34;&#34;
    # Se concatena título y selftext si es aplicable
    if &#34;selftext&#34; in data and data[&#34;selftext&#34;]:
        text = data[&#34;title&#34;] + &#34; &#34; + data[&#34;selftext&#34;]
    else:
        text = data[&#34;title&#34;]

    # Eliminar puntuación y espacios en blanco
    translator = str.maketrans(&#34;&#34;, &#34;&#34;, string.punctuation)
    text = text.translate(translator)
    text = re.sub(&#34;\s+&#34;, &#34; &#34;, text).lower()

    words = text.split()

    # Eliminamos palabras ridículamente largas ya que no tienen ningún sentido y además
    # pueden romper el Stemmer
    words = [word for word in words if len(word) &lt; 200]

    # Estematizar
    if stem:
        words = [ps.stem(word) for word in words]

    return words


def file_length(path):
    &#34;&#34;&#34;
        Cuenta el número de documentos en un fichero

        Parámetros
        ----------
        path: str
            \tRuta del documento

        Salida
        ------
        int
            \tNúmero de documentos en el fichero
    &#34;&#34;&#34;
    with open(path, encoding=&#34;UTF-8&#34;) as f:
        for i, _ in enumerate(f):
            pass
        return i+1


def parse_args():
    &#34;&#34;&#34;
        Procesamiento de los argumentos con los que se ejecutó el script
    &#34;&#34;&#34;
    parser = argparse.ArgumentParser(
        description=&#34;Script para entrenar modelos a partir de colecciones de documentos de Reddit&#34;)
    parser.add_argument(&#34;-training&#34;, default=&#34;datasets/training.ndjson&#34;,
                        help=&#34;Ruta del dataset de entrenamiento&#34;)
    parser.add_argument(&#34;-test&#34;, default=&#34;datasets/test.ndjson&#34;,
                        help=&#34;Ruta del dataset de test&#34;)
    parser.add_argument(&#34;-v&#34;, &#34;--vocab-size&#34;, default=5000, type=int, help=&#34;Número de términos a considerar en el vocabulario&#34;)
    parser.add_argument(&#34;-s&#34;, &#34;--seed&#34;, type=int, help=&#34;Semilla a utilizar&#34;)
    parser.add_argument(&#34;--stem&#34;, dest=&#34;stem&#34;, action=&#34;store_true&#34;,
                        help=&#34;Aplica estemetización al genererar el vocabulario&#34;)
    parser.add_argument(&#34;--no-stem&#34;, dest=&#34;stem&#34;, action=&#34;store_false&#34;,
                        help=&#34;No usa estemitazación en la generaciónd el vocabulario&#34;)
    parser.set_defaults(stem=True)

    return parser.parse_args()


if __name__ == &#34;__main__&#34;:
    main(parse_args())</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="src.classify.train.create_vocabulary"><code class="name flex">
<span>def <span class="ident">create_vocabulary</span></span>(<span>path, num_docs, stem)</span>
</code></dt>
<dd>
<div class="desc"><p>Genera una lista de n términos más frecuentes en el dataset proporcionado.
Los términos de este vocabulario formarán las características de cada individuo del dataset.</p>
<h2 id="parametros">Parámetros</h2>
<p>path: str<br>
Ruta del dataset a utilizar
num_docs: int<br>
Número de documentos del dataset
stem: bool<br>
Indica si se aplica o no estemetización en el preprocesado del texto</p>
<h2 id="salida">Salida</h2>
<p>list<br>
Lista de n términos más frecuentes del dataset</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_vocabulary(path, num_docs, stem):
    &#34;&#34;&#34;
        Genera una lista de n términos más frecuentes en el dataset proporcionado. 
        Los términos de este vocabulario formarán las características de cada individuo del dataset.

        Parámetros
        ----------
        path: str  
            \tRuta del dataset a utilizar
        num_docs: int  
            \tNúmero de documentos del dataset
        stem: bool  
            \tIndica si se aplica o no estemetización en el preprocesado del texto

        Salida
        ------
        list  
            \tLista de n términos más frecuentes del dataset
    &#34;&#34;&#34;
    word_list = []
    with open(path, encoding=&#34;UTF-8&#34;) as f:
        # Barra de progreso
        bar = pb.ProgressBar(max_value=num_docs)
        for line in bar(f):
            data = json.loads(line)[&#34;_source&#34;]
            # Preprocesamos el texto hasta obtener una lista de tokens
            word_list += preprocess_text(data, stem)

    # Nos quedamos con los VOCAB_SIZE términos más frecuentes
    vocabulary = Counter(word_list)
    vocabulary = vocabulary.most_common(VOCAB_SIZE)

    vocabulary_list = []
    for item, _ in vocabulary:
        vocabulary_list.append(item)

    return vocabulary_list</code></pre>
</details>
</dd>
<dt id="src.classify.train.extract_features"><code class="name flex">
<span>def <span class="ident">extract_features</span></span>(<span>path, vocabulary, num_docs, stem)</span>
</code></dt>
<dd>
<div class="desc"><p>Genera la matriz de características del dataset proporcionado.</p>
<p>Se itera a través de los documentos del dataset, de los que se extraen todos los términos del título y el selftext,
tras previo preprocesamiento. Para cada término que esté a su vez en el vocabulario generado anteriormente, se calcula
su frecuencia (tf) en el documento.</p>
<p>Por lo tanto, al final obtenemos una matriz donde cada fila es un documento y cada columna es el tf de uno de los términos del vocabulario.</p>
<h2 id="parametros">Parámetros</h2>
<p>path: str<br>
Ruta del dataset
vocabulary: list<br>
Lista de n términos más frecuentes en el dataset obtenida en create_vocabulary
num_docs: int<br>
Número de documentos en el dataset
stem: bool<br>
Indica si se aplica o no estemetización en el preprocesamiento del texto</p>
<h2 id="salida">Salida</h2>
<p>ndarray<br>
Matriz con el tf de cada palabra en cada documento</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def extract_features(path, vocabulary, num_docs, stem):
    &#34;&#34;&#34;
        Genera la matriz de características del dataset proporcionado.

        Se itera a través de los documentos del dataset, de los que se extraen todos los términos del título y el selftext, 
        tras previo preprocesamiento. Para cada término que esté a su vez en el vocabulario generado anteriormente, se calcula 
        su frecuencia (tf) en el documento.

        Por lo tanto, al final obtenemos una matriz donde cada fila es un documento y cada columna es el tf de uno de los términos del vocabulario.


        Parámetros
        ----------
        path: str  
            \tRuta del dataset
        vocabulary: list  
            \tLista de n términos más frecuentes en el dataset obtenida en create_vocabulary
        num_docs: int  
            \tNúmero de documentos en el dataset
        stem: bool  
            \tIndica si se aplica o no estemetización en el preprocesamiento del texto

        Salida
        ------
        ndarray  
            \tMatriz con el tf de cada palabra en cada documento       
        
    &#34;&#34;&#34;

    &#34;&#34;&#34;
        La matriz de características puede llegar a tener tamaños problemáticos debido a las limitaciones de hardware que pudiéramos tener.
        Por otra parte, la naturaleza de la matriz de características en nuestro problema implica que estamos trabajando con una matriz donde
        la mayor parte de los valores son 0s (cada término del vocabulario que no aparezca en un documento va tener una frecuencia 0)
        Una matriz donde la mayor parte de los valores son 0 es lo que se conoce como matriz dispersa, la cuál podemos almacenar en una estructura 
        de datos especializada, como lo es [csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html#scipy.sparse.csr_matrix) dentro del paquete `scipy`
        Utilizando esta estructura en vez de un array de numpy podemos llegar a ahorrar mucha memoria.
    &#34;&#34;&#34;
    # Necesitamos llevar estos tres arrays para crear la matriz dispersa al acabar el procesado
    indices = [] # Índices de los términos
    values = [] # Las frecuencias
    indptr = [] # Punteros que indican donde empieza cada documento

    bar = pb.ProgressBar(max_value=num_docs)
    indptr.append(0)
    with open(path, encoding=&#34;UTF-8&#34;) as f:
        for line in bar(f):
            data = json.loads(line)[&#34;_source&#34;]
            word_list = preprocess_text(data, stem)

            features = {}
            for word in word_list:
                if word in vocabulary:
                    vocab_index = vocabulary.index(word)
                    if word in features:
                        features[vocab_index] += 1
                    else:
                        features[vocab_index] = 1 
            
            indices.extend(features.keys())
            values.extend(features.values())
            indptr.append(len(indices))

    # Interesante jugar con el tipo numérico para ahorrar memoria. 
    indices = np.asarray(indices, dtype=np.int32)
    indptr = np.asarray(indptr, dtype=np.int32)
    values = np.asarray(values, dtype=np.int32)

    matrix = sp.csr_matrix((values, indices, indptr),
        shape=(len(indptr) -1, len(vocabulary)),
        dtype= np.int32)
    matrix.sort_indices()

    return matrix</code></pre>
</details>
</dd>
<dt id="src.classify.train.file_length"><code class="name flex">
<span>def <span class="ident">file_length</span></span>(<span>path)</span>
</code></dt>
<dd>
<div class="desc"><p>Cuenta el número de documentos en un fichero</p>
<h2 id="parametros">Parámetros</h2>
<p>path: str
Ruta del documento</p>
<h2 id="salida">Salida</h2>
<p>int
Número de documentos en el fichero</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def file_length(path):
    &#34;&#34;&#34;
        Cuenta el número de documentos en un fichero

        Parámetros
        ----------
        path: str
            \tRuta del documento

        Salida
        ------
        int
            \tNúmero de documentos en el fichero
    &#34;&#34;&#34;
    with open(path, encoding=&#34;UTF-8&#34;) as f:
        for i, _ in enumerate(f):
            pass
        return i+1</code></pre>
</details>
</dd>
<dt id="src.classify.train.get_tags"><code class="name flex">
<span>def <span class="ident">get_tags</span></span>(<span>path, num_docs)</span>
</code></dt>
<dd>
<div class="desc"><p>Extrae los valores de la etiqueta a clasificar del dataset especificado</p>
<h2 id="parametros">Parámetros</h2>
<p>path: str<br>
Ruta del dataset
num_docs: int<br>
Número de documentos en el dataset</p>
<h2 id="salida">Salida</h2>
<p>ndarray<br>
Matriz con las etiquetas del dataset especificado</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_tags(path, num_docs):
    &#34;&#34;&#34;
        Extrae los valores de la etiqueta a clasificar del dataset especificado

        Parámetros
        ----------
        path: str  
            \tRuta del dataset
        num_docs: int  
            \tNúmero de documentos en el dataset

        Salida
        ------
        ndarray  
            \tMatriz con las etiquetas del dataset especificado
    &#34;&#34;&#34;

    # Tipo int16 por limitaciones de memoria
    matrix = np.zeros((num_docs), dtype=np.int16)
    doc_id = 0

    bar = pb.ProgressBar(max_value=num_docs)
    with open(path, encoding=&#34;UTF-8&#34;) as f:
        for line in bar(f):
            tag = json.loads(line)[&#34;_source&#34;][&#34;lonely&#34;]
            matrix[doc_id] = int(tag)
            doc_id +=1
    return matrix</code></pre>
</details>
</dd>
<dt id="src.classify.train.main"><code class="name flex">
<span>def <span class="ident">main</span></span>(<span>args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def main(args):
    global VOCAB_SIZE
    VOCAB_SIZE = args.vocab_size

    if args.seed:
        np.random.seed(args.seed)
        
    num_training_docs = file_length(args.training)
    
    # Comentando o descomentando los bloques indicados podemos serializar los resultados parciales
    # del preprocesamiento previo al entrenamiento en disco, para ahorrarnos estos pasos en ejecuciones posteriores

    print(&#34;Generando el vocabulario...&#34;)
    vocabulary = create_vocabulary(args.training, num_training_docs, args.stem)    
    # De esta forma, podemos serializar el vocabulario para no tener que recalcularlo cuando entrenemos otros modelos
    joblib.dump(vocabulary, &#34;pickles/vocabulary.pickle&#34;)
    &#34;&#34;&#34;
    # Si tenemos el vocabulario ya en disco, se cargaría así    
    vocabulary = joblib.load(&#34;pickles/vocabulary.pickle&#34;)    
    &#34;&#34;&#34;
    print(&#34;Obteniendo etiquetas...&#34;)
    tags_training = get_tags(args.training, num_training_docs)
    # Serializar etiquetas
    joblib.dump(tags_training, &#34;pickles/tags.pickle&#34;)    
    &#34;&#34;&#34;    
    # Cargarlas desde disco
    tags_training = joblib.load(&#34;pickles/tags.pickle&#34;)
    &#34;&#34;&#34;
    print(&#34;Extrayendo características...&#34;)
    matrix_training = extract_features(args.training, vocabulary, num_training_docs, args.stem)
    # Serializar
    sp.save_npz(&#34;pickles/features.npz&#34;, matrix_training)
    &#34;&#34;&#34;
    # Cargar desde disco
    matrix_training = sp.load_npz(&#34;pickles/features.npz&#34;)
    &#34;&#34;&#34;

    print(&#34;Entrenando modelos...&#34;)
    # Metemos aquí los modelos que queramos entrenar en esta ejecución del script
    models = {
        &#34;LogisticRegression&#34;: LogisticRegression(random_state=args.seed),
        &#34;GiniTree&#34;: DecisionTreeClassifier(random_state=args.seed),
        &#34;IDFTree&#34;: DecisionTreeClassifier(criterion=&#34;entropy&#34;, random_state=args.seed),
        &#34;Bayes&#34;: MultinomialNB(),
        &#34;NN_1L_150&#34;: MLPClassifier(hidden_layer_sizes=(150), random_state=args.seed),
        &#34;NN_2L_150&#34;: MLPClassifier(hidden_layer_sizes=(150,150), random_state=args.seed),
        &#34;NN_1L_300&#34;: MLPClassifier(hidden_layer_sizes=(300), random_state=args.seed),
        &#34;SVM_Linear&#34;: svm.LinearSVC()
    }

    ## Ejemplos
    # model = LogisticRegression(verbose=True)
    # model = MultinomialNB() # Red Bayesiana
    # model = DecisionTreeClassifier()
    # model = DecisionTreeClassifier(criterion=&#34;entropy&#34;)
    # model = MLPClassifier(hidden_layer_sizes=(150), random_state=args.seed, verbose=True)
    # model = svm.LinearSVC(verbose=True)
    # model = svm.SVC(cache_size=10000, verbose=True) # TODO Probar parámetro cache_size
    
    # Para entrenar los modelos secuencialmente
    train_models(models, matrix_training, tags_training, n_jobs=8)

    print(&#34;Evaluando modelos...&#34;)
    num_test_docs = file_length(args.test)
    print(&#34;Obteniendo etiquetas del conjunto de test...&#34;)    
    tags_test = get_tags(args.test, num_test_docs)   
    # Serializar
    joblib.dump(tags_test, &#34;pickles/tags_test.pickle&#34;)
    &#34;&#34;&#34;
    # Alternativamente, cargar de disco
    tags_test = joblib.load(&#34;pickles/tags_test.pickle&#34;)
    &#34;&#34;&#34;
    print(&#34;Extrayendo características del conjunto de test...&#34;)   
    matrix_test = extract_features(args.test, vocabulary, num_test_docs, args.stem)
    # Serializar
    sp.save_npz(&#34;pickles/features_test.npz&#34;, matrix_test)    
    &#34;&#34;&#34;
    # Cargar de disco
    matrix_test = sp.load_npz(&#34;pickles/features_test.npz&#34;)    
    &#34;&#34;&#34;

    print(&#34;Generando predicciones para el conjunto de test...&#34;)
    for model in models:
        print(&#34;Modelo &#34; + model + &#34;...&#34;)
        results = models[model].predict(matrix_test)
        pprint(get_stats(results, tags_test))</code></pre>
</details>
</dd>
<dt id="src.classify.train.parse_args"><code class="name flex">
<span>def <span class="ident">parse_args</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Procesamiento de los argumentos con los que se ejecutó el script</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse_args():
    &#34;&#34;&#34;
        Procesamiento de los argumentos con los que se ejecutó el script
    &#34;&#34;&#34;
    parser = argparse.ArgumentParser(
        description=&#34;Script para entrenar modelos a partir de colecciones de documentos de Reddit&#34;)
    parser.add_argument(&#34;-training&#34;, default=&#34;datasets/training.ndjson&#34;,
                        help=&#34;Ruta del dataset de entrenamiento&#34;)
    parser.add_argument(&#34;-test&#34;, default=&#34;datasets/test.ndjson&#34;,
                        help=&#34;Ruta del dataset de test&#34;)
    parser.add_argument(&#34;-v&#34;, &#34;--vocab-size&#34;, default=5000, type=int, help=&#34;Número de términos a considerar en el vocabulario&#34;)
    parser.add_argument(&#34;-s&#34;, &#34;--seed&#34;, type=int, help=&#34;Semilla a utilizar&#34;)
    parser.add_argument(&#34;--stem&#34;, dest=&#34;stem&#34;, action=&#34;store_true&#34;,
                        help=&#34;Aplica estemetización al genererar el vocabulario&#34;)
    parser.add_argument(&#34;--no-stem&#34;, dest=&#34;stem&#34;, action=&#34;store_false&#34;,
                        help=&#34;No usa estemitazación en la generaciónd el vocabulario&#34;)
    parser.set_defaults(stem=True)

    return parser.parse_args()</code></pre>
</details>
</dd>
<dt id="src.classify.train.preprocess_text"><code class="name flex">
<span>def <span class="ident">preprocess_text</span></span>(<span>data, stem)</span>
</code></dt>
<dd>
<div class="desc"><p>Se extrae el título y el contenido de un post y se realiza un preprocesado del texto, eliminando
puntuación, espacios en blanco y si se indica, aplicando un stemmer</p>
<h2 id="parametros">Parámetros</h2>
<p>data: json<br>
Documento con el contenido de un post de Reddit
stem: bool<br>
Indica si se aplica o no estemetización al texto</p>
<h2 id="salida">Salida</h2>
<p>list<br>
Lista de palabras del documento preprocesadas</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preprocess_text(data, stem):
    &#34;&#34;&#34;
        Se extrae el título y el contenido de un post y se realiza un preprocesado del texto, eliminando
        puntuación, espacios en blanco y si se indica, aplicando un stemmer

        Parámetros
        ----------
        data: json  
            \tDocumento con el contenido de un post de Reddit
        stem: bool  
            \tIndica si se aplica o no estemetización al texto

        Salida
        ------
        list  
            \tLista de palabras del documento preprocesadas
    &#34;&#34;&#34;
    # Se concatena título y selftext si es aplicable
    if &#34;selftext&#34; in data and data[&#34;selftext&#34;]:
        text = data[&#34;title&#34;] + &#34; &#34; + data[&#34;selftext&#34;]
    else:
        text = data[&#34;title&#34;]

    # Eliminar puntuación y espacios en blanco
    translator = str.maketrans(&#34;&#34;, &#34;&#34;, string.punctuation)
    text = text.translate(translator)
    text = re.sub(&#34;\s+&#34;, &#34; &#34;, text).lower()

    words = text.split()

    # Eliminamos palabras ridículamente largas ya que no tienen ningún sentido y además
    # pueden romper el Stemmer
    words = [word for word in words if len(word) &lt; 200]

    # Estematizar
    if stem:
        words = [ps.stem(word) for word in words]

    return words</code></pre>
</details>
</dd>
<dt id="src.classify.train.train_models"><code class="name flex">
<span>def <span class="ident">train_models</span></span>(<span>models, features, tags, n_jobs=1)</span>
</code></dt>
<dd>
<div class="desc"><p>Entrena un conjunto de modelos.</p>
<p>Por lo general, los modelos de scikit-learn se entrenan en un único hilo. En su lugar, lo que podemos hacer
es entrenar varios modelos, cada uno de ellos en un hilo diferente. Para ello se utiliza el parámetro n_jobs
de esta función. Si éste es 1, los modelos se entrenarán secuencialmente. Si es mayor de 1, se entrenarán en paralelo
utilizando tantos hilos como se hayan especificado.</p>
<h2 id="parametros">Parámetros</h2>
<p>models: dict<br>
Diccionario con los modelos a entrenar<br>
features: csr_matrix<br>
Matriz dispersa con las características del conjunto de entrenamiento<br>
tags: np.array<br>
Matriz con las etiquetas del conjunto de entrenamiento<br>
n_jobs: int<br>
Número de hilos a utilizar. </p>
<h2 id="salida">Salida</h2>
<p>models: dict
Diccionario con los modelos ya entrenados</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_models(models, features, tags, n_jobs = 1):
    &#34;&#34;&#34;
        Entrena un conjunto de modelos.
        
        Por lo general, los modelos de scikit-learn se entrenan en un único hilo. En su lugar, lo que podemos hacer 
        es entrenar varios modelos, cada uno de ellos en un hilo diferente. Para ello se utiliza el parámetro n_jobs
        de esta función. Si éste es 1, los modelos se entrenarán secuencialmente. Si es mayor de 1, se entrenarán en paralelo
        utilizando tantos hilos como se hayan especificado.

        Parámetros
        ----------
        models: dict  
            \tDiccionario con los modelos a entrenar  
        features: csr_matrix  
            \tMatriz dispersa con las características del conjunto de entrenamiento  
        tags: np.array  
            \tMatriz con las etiquetas del conjunto de entrenamiento  
        n_jobs: int  
            \tNúmero de hilos a utilizar. 
        
        Salida
        ------
        models: dict
            \tDiccionario con los modelos ya entrenados
    &#34;&#34;&#34;
    if n_jobs &gt; 1:
        # [joblib.Parallel](https://joblib.readthedocs.io/en/latest/generated/joblib.Parallel.html) nos permite entrenar varios modelos concurrentemente
        results = joblib.Parallel(n_jobs=n_jobs)(joblib.delayed(__train_model)(model, models[model], features, tags) for model in models)
        for name, trained_model in results:
            models[name] = trained_model
    else:
        for model in models:
            models[model] = __train_model(model, models[model], features, tags)[1]
    return models</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#script-para-entrenar-un-clasificador-de-post-de-reddit">Script para entrenar un clasificador de post de Reddit</a></li>
<li><a href="#parametros">Parámetros</a></li>
</ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src.classify" href="index.html">src.classify</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="src.classify.train.create_vocabulary" href="#src.classify.train.create_vocabulary">create_vocabulary</a></code></li>
<li><code><a title="src.classify.train.extract_features" href="#src.classify.train.extract_features">extract_features</a></code></li>
<li><code><a title="src.classify.train.file_length" href="#src.classify.train.file_length">file_length</a></code></li>
<li><code><a title="src.classify.train.get_tags" href="#src.classify.train.get_tags">get_tags</a></code></li>
<li><code><a title="src.classify.train.main" href="#src.classify.train.main">main</a></code></li>
<li><code><a title="src.classify.train.parse_args" href="#src.classify.train.parse_args">parse_args</a></code></li>
<li><code><a title="src.classify.train.preprocess_text" href="#src.classify.train.preprocess_text">preprocess_text</a></code></li>
<li><code><a title="src.classify.train.train_models" href="#src.classify.train.train_models">train_models</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.1</a>.</p>
</footer>
</body>
</html>