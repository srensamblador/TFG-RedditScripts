<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.7.4" />
<title>src.posts_from_scales API documentation</title>
<meta name="description" content="Script para la extracción e indexado de posts de Reddit relevantes a items de escalas psicométricas
…" />
<link href='https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.0/normalize.min.css' rel='stylesheet'>
<link href='https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/8.0.0/sanitize.min.css' rel='stylesheet'>
<link href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css" rel="stylesheet">
<style>.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{font-weight:bold}#index h4 + ul{margin-bottom:.6em}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>src.posts_from_scales</code></h1>
</header>
<section id="section-intro">
<h2 id="script-para-la-extraccion-e-indexado-de-posts-de-reddit-relevantes-a-items-de-escalas-psicometricas">Script para la extracción e indexado de posts de Reddit relevantes a items de escalas psicométricas</h2>
<p>Script encargado de emplear la API de Pushshift para extraer el histórico de posts de Reddit que sean relevantes
para un item de una escala psicométrica de soledad. <br>
Se vuelcan los documentos obtenidos en ficheros .json.<br>
Se indexan en Elasticsearch, previamente añadiendo campos para identificar la frase y escala con la que se obtuvieron los documentos,
además de añadiendo un booleano para marcar los documentos como positivos en alguna escala de soledad.</p>
<p>Para ejecutar el script se necesita un fichero de texto con frases y escalas separadas por líneas siguiendo el siguiente formato:</p>
<pre><code>Frase 1;Escala 1  
Frase 2;Escala 2  
Frase 3;Escala 3  
...
</code></pre>
<h2 id="parametros">Parámetros</h2>
<ul>
<li>-q, &ndash;query-file: fichero de texto desde el que se cargan las frases. Por defecto, frases.txt</li>
<li>-d, &ndash;dump-dir: directorio donde se volcarán los ficheros .json. Por defecto /dumps</li>
<li>-e, &ndash;elasticsearch: dirección del servidor Elasticsearch contra el que se indexará. Por defecto <a href="http://localhost:9200">http://localhost:9200</a></li>
<li>-b, &ndash;before: fecha donde se comenzará a extraer posts hacia atrás en el tiempo. Por defecto, la fecha actual.</li>
</ul>
<p>Ejemplo de línea de ejecución.</p>
<pre><code>$ python main.py -q frases.txt -d dumps -e &lt;http://localhost:9200&gt; -b 2019-12-31
</code></pre>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#!/usr/bin/env python
&#34;&#34;&#34;
    Script para la extracción e indexado de posts de Reddit relevantes a items de escalas psicométricas
    ----------------------------------------------------------------------------------------------

    Script encargado de emplear la API de Pushshift para extraer el histórico de posts de Reddit que sean relevantes
    para un item de una escala psicométrica de soledad.   
    Se vuelcan los documentos obtenidos en ficheros .json.  
    Se indexan en Elasticsearch, previamente añadiendo campos para identificar la frase y escala con la que se obtuvieron los documentos,
    además de añadiendo un booleano para marcar los documentos como positivos en alguna escala de soledad.

    Para ejecutar el script se necesita un fichero de texto con frases y escalas separadas por líneas siguiendo el siguiente formato:

        Frase 1;Escala 1  
        Frase 2;Escala 2  
        Frase 3;Escala 3  
        ...  

    Parámetros
    ----------
    * -q, --query-file: fichero de texto desde el que se cargan las frases. Por defecto, frases.txt
    * -d, --dump-dir: directorio donde se volcarán los ficheros .json. Por defecto /dumps
    * -e, --elasticsearch: dirección del servidor Elasticsearch contra el que se indexará. Por defecto http://localhost:9200
    * -b, --before: fecha donde se comenzará a extraer posts hacia atrás en el tiempo. Por defecto, la fecha actual.

    Ejemplo de línea de ejecución.

        $ python main.py -q frases.txt -d dumps -e http://localhost:9200 -b 2019-12-31
&#34;&#34;&#34;

from psaw import PushshiftAPI
import argparse
from  datetime import datetime
import json
import os
from elasticsearch import Elasticsearch
from src.elastic_utils.elastic_indexers import Indexer, NgramIndexer

__author__ = &#34;Samuel Cifuentes García&#34;

def main(args):
    # Establece la conexión a Elastic
    global es
    es = Elasticsearch(args.elasticsearch)

    # Inicializamos el cliente de la API
    global api
    api = PushshiftAPI()

    # Se cargan las frases a procesar desde el fichero pasado por parámetro
    queries = load_queries(args.query_file)

    # Crea un directorio para los volcados si no existe
    if not os.path.exists(args.dump_dir):
        os.makedirs(args.dump_dir)

    for query, scale in queries.items():
        # Crea el .json de backup donde se volcarán los post
        global dump_filename
        dump_filename = args.dump_dir + &#34;/&#34; + query.replace(&#34; &#34; ,&#34;&#34;) + &#34;-Dump.json&#34;
        print(&#34;Procesando frase: \&#34;&#34; + query + &#34;\&#34;...&#34;)

        query_API(query, scale, args.before, cache_size=100)

        print(&#34;Frase completada: \&#34;&#34;+ query + &#34;\&#34;&#34;)

def load_queries(filename):
    &#34;&#34;&#34;
        Carga las frases a consultar e indexar desde un fichero de texto pasado por parámetro.
        
        Parámetros
        ----------
        filename: str
            fichero de texto con las frases a utilizar en la ejecución del programa

    &#34;&#34;&#34;
    queries = {}

    with open(filename) as f:
        for line in f:
            query = line.strip().split(&#34;;&#34;)
            queries[query[0]] = query[1]
    
    return queries

def query_API(query, scale,  before_timestamp, cache_size = 3000):
    &#34;&#34;&#34;
        Se utiliza psaw (https://github.com/dmarx/psaw) para consumir la API de Pushshift y extraer submissions. 
        A cada submission se le añaden tres campos: la frase con la que se obtuvo, la escala a la que pertenece la frase y
        un booleano para indicar que el post dio positivo en una escala.

        Parámetros
        ----------
        query: str
            Frase a consultar contra la API
        scale: str
            Escala a la que pertenece la frase
        cache_size: int
            Opcional. Cada cuantos documentos se realiza un volcado e indexado
        start_date: Date

    &#34;&#34;&#34;
    gen = api.search_submissions(q=query, before=before_timestamp)
    cache = []
    
    numIter = 0 
    for c in gen:
        c.d_[&#34;query&#34;] = query
        c.d_[&#34;scale&#34;] = scale
        c.d_[&#34;lonely&#34;] = True        
        cache.append(c.d_)

        if len(cache) == cache_size:
            
            dump_to_file(cache)
            elastic_index(cache, query, scale)
            
            print(&#34; *&#34;, datetime.fromtimestamp(cache[-1][&#34;created_utc&#34;]).strftime(&#34;%Y-%m-%d&#34;))

            cache = []

            numIter += 1
        
    dump_to_file(cache)
    elastic_index(cache, query, scale)

def dump_to_file(results):
    &#34;&#34;&#34;
        Vuelca una lista de submissions a un fichero .json. Los volcados se deben realizar de forma parcial 
        debido a limitaciones de memoria.
        Las escrituras son mucho más rápidas si se tratan como strings en vez de objetos JSON.

        Parámetros
        ----------
        results: list
            lista de documentos a volcar   
    &#34;&#34;&#34;
    with open(dump_filename, &#34;a&#34;) as f:
        for result in results:
            f.write(json.dumps(result) + &#34;\n&#34;)

def elastic_index(results, query, scale):
    &#34;&#34;&#34;
        Indexa la lista de documentos pasados por parámetro. 
        Se crean dos índices si no existen, uno para unigramas y otro para bigramas. 
        A cada documento se le añadirá tres campos: la consulta y escala utilizadas para obtenerlo y 
        un booleano para marcarlos como positivos en soledad.

        Parámetros
        ----------
        results: list
            lista de documentos a indexar
        query: str
            frase utilizada para extraer los documentos
        scale: str
            escala psicométrica de la que proviene la frase
    &#34;&#34;&#34;
    indexers = [Indexer(es, &#34;reddit-loneliness&#34;), NgramIndexer(es, &#34;reddit-loneliness-ngram&#34;)]
    for indexer in indexers:
        if not indexer.index_exists():
            print(&#34;Creado índice: &#34; + indexer.index_name)
            indexer.create_index()
        
        indexer.index_documents(results)

def parse_args():
    &#34;&#34;&#34;
        Procesamiento de los argumentos con los que se ejecutó el script
    &#34;&#34;&#34;
    parser = argparse.ArgumentParser(description=&#34;Script para la extracción de submissions de Reddit a través de pushshift.io&#34;)
    parser.add_argument(&#34;-q&#34;, &#34;--query-file&#34;, default=&#34;frases.txt&#34;, help=&#34;Fichero con las frases a consultar.&#34;)
    parser.add_argument(&#34;-d&#34;, &#34;--dump-dir&#34;, default=&#34;dumps&#34;, help=&#34;Directorio donde se volcarán los archivos .json de backup&#34;)
    parser.add_argument(&#34;-e&#34;, &#34;--elasticsearch&#34;, default=&#34;http://localhost:9200&#34;, help=&#34;dirección del servidor Elasticsearch contra el que se indexará&#34;)
    parser.add_argument(&#34;-b&#34;, &#34;--before&#34;, default=datetime.date.today(), 
    type= lambda d: datetime.datetime.strptime(d, &#39;%Y-%m-%d&#39;).date(), 
    help=&#34;timestamp desde el que se empezará a recuperar documentos hacia atrás en formato YYYY-mm-dd&#34;)
    return parser.parse_args()

if __name__ == &#34;__main__&#34;:
    main(parse_args())</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="src.posts_from_scales.dump_to_file"><code class="name flex">
<span>def <span class="ident">dump_to_file</span></span>(<span>results)</span>
</code></dt>
<dd>
<section class="desc"><p>Vuelca una lista de submissions a un fichero .json. Los volcados se deben realizar de forma parcial
debido a limitaciones de memoria.
Las escrituras son mucho más rápidas si se tratan como strings en vez de objetos JSON.</p>
<h2 id="parametros">Parámetros</h2>
<dl>
<dt><strong><code>results</code></strong> :&ensp;<code>list</code></dt>
<dd>lista de documentos a volcar</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dump_to_file(results):
    &#34;&#34;&#34;
        Vuelca una lista de submissions a un fichero .json. Los volcados se deben realizar de forma parcial 
        debido a limitaciones de memoria.
        Las escrituras son mucho más rápidas si se tratan como strings en vez de objetos JSON.

        Parámetros
        ----------
        results: list
            lista de documentos a volcar   
    &#34;&#34;&#34;
    with open(dump_filename, &#34;a&#34;) as f:
        for result in results:
            f.write(json.dumps(result) + &#34;\n&#34;)</code></pre>
</details>
</dd>
<dt id="src.posts_from_scales.elastic_index"><code class="name flex">
<span>def <span class="ident">elastic_index</span></span>(<span>results, query, scale)</span>
</code></dt>
<dd>
<section class="desc"><p>Indexa la lista de documentos pasados por parámetro.
Se crean dos índices si no existen, uno para unigramas y otro para bigramas.
A cada documento se le añadirá tres campos: la consulta y escala utilizadas para obtenerlo y
un booleano para marcarlos como positivos en soledad.</p>
<h2 id="parametros">Parámetros</h2>
<dl>
<dt><strong><code>results</code></strong> :&ensp;<code>list</code></dt>
<dd>lista de documentos a indexar</dd>
<dt><strong><code>query</code></strong> :&ensp;<code>str</code></dt>
<dd>frase utilizada para extraer los documentos</dd>
<dt><strong><code>scale</code></strong> :&ensp;<code>str</code></dt>
<dd>escala psicométrica de la que proviene la frase</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def elastic_index(results, query, scale):
    &#34;&#34;&#34;
        Indexa la lista de documentos pasados por parámetro. 
        Se crean dos índices si no existen, uno para unigramas y otro para bigramas. 
        A cada documento se le añadirá tres campos: la consulta y escala utilizadas para obtenerlo y 
        un booleano para marcarlos como positivos en soledad.

        Parámetros
        ----------
        results: list
            lista de documentos a indexar
        query: str
            frase utilizada para extraer los documentos
        scale: str
            escala psicométrica de la que proviene la frase
    &#34;&#34;&#34;
    indexers = [Indexer(es, &#34;reddit-loneliness&#34;), NgramIndexer(es, &#34;reddit-loneliness-ngram&#34;)]
    for indexer in indexers:
        if not indexer.index_exists():
            print(&#34;Creado índice: &#34; + indexer.index_name)
            indexer.create_index()
        
        indexer.index_documents(results)</code></pre>
</details>
</dd>
<dt id="src.posts_from_scales.load_queries"><code class="name flex">
<span>def <span class="ident">load_queries</span></span>(<span>filename)</span>
</code></dt>
<dd>
<section class="desc"><p>Carga las frases a consultar e indexar desde un fichero de texto pasado por parámetro.</p>
<h2 id="parametros">Parámetros</h2>
<dl>
<dt><strong><code>filename</code></strong> :&ensp;<code>str</code></dt>
<dd>fichero de texto con las frases a utilizar en la ejecución del programa</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_queries(filename):
    &#34;&#34;&#34;
        Carga las frases a consultar e indexar desde un fichero de texto pasado por parámetro.
        
        Parámetros
        ----------
        filename: str
            fichero de texto con las frases a utilizar en la ejecución del programa

    &#34;&#34;&#34;
    queries = {}

    with open(filename) as f:
        for line in f:
            query = line.strip().split(&#34;;&#34;)
            queries[query[0]] = query[1]
    
    return queries</code></pre>
</details>
</dd>
<dt id="src.posts_from_scales.main"><code class="name flex">
<span>def <span class="ident">main</span></span>(<span>args)</span>
</code></dt>
<dd>
<section class="desc"></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def main(args):
    # Establece la conexión a Elastic
    global es
    es = Elasticsearch(args.elasticsearch)

    # Inicializamos el cliente de la API
    global api
    api = PushshiftAPI()

    # Se cargan las frases a procesar desde el fichero pasado por parámetro
    queries = load_queries(args.query_file)

    # Crea un directorio para los volcados si no existe
    if not os.path.exists(args.dump_dir):
        os.makedirs(args.dump_dir)

    for query, scale in queries.items():
        # Crea el .json de backup donde se volcarán los post
        global dump_filename
        dump_filename = args.dump_dir + &#34;/&#34; + query.replace(&#34; &#34; ,&#34;&#34;) + &#34;-Dump.json&#34;
        print(&#34;Procesando frase: \&#34;&#34; + query + &#34;\&#34;...&#34;)

        query_API(query, scale, args.before, cache_size=100)

        print(&#34;Frase completada: \&#34;&#34;+ query + &#34;\&#34;&#34;)</code></pre>
</details>
</dd>
<dt id="src.posts_from_scales.parse_args"><code class="name flex">
<span>def <span class="ident">parse_args</span></span>(<span>)</span>
</code></dt>
<dd>
<section class="desc"><p>Procesamiento de los argumentos con los que se ejecutó el script</p></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse_args():
    &#34;&#34;&#34;
        Procesamiento de los argumentos con los que se ejecutó el script
    &#34;&#34;&#34;
    parser = argparse.ArgumentParser(description=&#34;Script para la extracción de submissions de Reddit a través de pushshift.io&#34;)
    parser.add_argument(&#34;-q&#34;, &#34;--query-file&#34;, default=&#34;frases.txt&#34;, help=&#34;Fichero con las frases a consultar.&#34;)
    parser.add_argument(&#34;-d&#34;, &#34;--dump-dir&#34;, default=&#34;dumps&#34;, help=&#34;Directorio donde se volcarán los archivos .json de backup&#34;)
    parser.add_argument(&#34;-e&#34;, &#34;--elasticsearch&#34;, default=&#34;http://localhost:9200&#34;, help=&#34;dirección del servidor Elasticsearch contra el que se indexará&#34;)
    parser.add_argument(&#34;-b&#34;, &#34;--before&#34;, default=datetime.date.today(), 
    type= lambda d: datetime.datetime.strptime(d, &#39;%Y-%m-%d&#39;).date(), 
    help=&#34;timestamp desde el que se empezará a recuperar documentos hacia atrás en formato YYYY-mm-dd&#34;)
    return parser.parse_args()</code></pre>
</details>
</dd>
<dt id="src.posts_from_scales.query_API"><code class="name flex">
<span>def <span class="ident">query_API</span></span>(<span>query, scale, before_timestamp, cache_size=3000)</span>
</code></dt>
<dd>
<section class="desc"><p>Se utiliza psaw (<a href="https://github.com/dmarx/psaw">https://github.com/dmarx/psaw</a>) para consumir la API de Pushshift y extraer submissions.
A cada submission se le añaden tres campos: la frase con la que se obtuvo, la escala a la que pertenece la frase y
un booleano para indicar que el post dio positivo en una escala.</p>
<h2 id="parametros">Parámetros</h2>
<dl>
<dt><strong><code>query</code></strong> :&ensp;<code>str</code></dt>
<dd>Frase a consultar contra la API</dd>
<dt><strong><code>scale</code></strong> :&ensp;<code>str</code></dt>
<dd>Escala a la que pertenece la frase</dd>
<dt><strong><code>cache_size</code></strong> :&ensp;<code>int</code></dt>
<dd>Opcional. Cada cuantos documentos se realiza un volcado e indexado</dd>
<dt><strong><code>start_date</code></strong> :&ensp;<code>Date</code></dt>
<dd>&nbsp;</dd>
</dl></section>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def query_API(query, scale,  before_timestamp, cache_size = 3000):
    &#34;&#34;&#34;
        Se utiliza psaw (https://github.com/dmarx/psaw) para consumir la API de Pushshift y extraer submissions. 
        A cada submission se le añaden tres campos: la frase con la que se obtuvo, la escala a la que pertenece la frase y
        un booleano para indicar que el post dio positivo en una escala.

        Parámetros
        ----------
        query: str
            Frase a consultar contra la API
        scale: str
            Escala a la que pertenece la frase
        cache_size: int
            Opcional. Cada cuantos documentos se realiza un volcado e indexado
        start_date: Date

    &#34;&#34;&#34;
    gen = api.search_submissions(q=query, before=before_timestamp)
    cache = []
    
    numIter = 0 
    for c in gen:
        c.d_[&#34;query&#34;] = query
        c.d_[&#34;scale&#34;] = scale
        c.d_[&#34;lonely&#34;] = True        
        cache.append(c.d_)

        if len(cache) == cache_size:
            
            dump_to_file(cache)
            elastic_index(cache, query, scale)
            
            print(&#34; *&#34;, datetime.fromtimestamp(cache[-1][&#34;created_utc&#34;]).strftime(&#34;%Y-%m-%d&#34;))

            cache = []

            numIter += 1
        
    dump_to_file(cache)
    elastic_index(cache, query, scale)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul>
<li><a href="#script-para-la-extraccion-e-indexado-de-posts-de-reddit-relevantes-a-items-de-escalas-psicometricas">Script para la extracción e indexado de posts de Reddit relevantes a items de escalas psicométricas</a></li>
<li><a href="#parametros">Parámetros</a></li>
</ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="src" href="index.html">src</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="two-column">
<li><code><a title="src.posts_from_scales.dump_to_file" href="#src.posts_from_scales.dump_to_file">dump_to_file</a></code></li>
<li><code><a title="src.posts_from_scales.elastic_index" href="#src.posts_from_scales.elastic_index">elastic_index</a></code></li>
<li><code><a title="src.posts_from_scales.load_queries" href="#src.posts_from_scales.load_queries">load_queries</a></code></li>
<li><code><a title="src.posts_from_scales.main" href="#src.posts_from_scales.main">main</a></code></li>
<li><code><a title="src.posts_from_scales.parse_args" href="#src.posts_from_scales.parse_args">parse_args</a></code></li>
<li><code><a title="src.posts_from_scales.query_API" href="#src.posts_from_scales.query_API">query_API</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.7.4</a>.</p>
</footer>
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
<script>hljs.initHighlightingOnLoad()</script>
</body>
</html>